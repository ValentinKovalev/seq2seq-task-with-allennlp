{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/title-generation-readers/test_reader.p\n",
      "/kaggle/input/title-generation-readers/train_reader.p\n",
      "/kaggle/input/allen-nlp-baseline/__notebook__.ipynb\n",
      "/kaggle/input/allen-nlp-baseline/res.csv\n",
      "/kaggle/input/allen-nlp-baseline/submission.csv\n",
      "/kaggle/input/allen-nlp-baseline/__results__.html\n",
      "/kaggle/input/allen-nlp-baseline/model.h5\n",
      "/kaggle/input/allen-nlp-baseline/__output__.json\n",
      "/kaggle/input/allen-nlp-baseline/custom.css\n",
      "/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\n",
      "/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.50d.txt\n",
      "/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\n",
      "/kaggle/input/title-generation/test.csv\n",
      "/kaggle/input/title-generation/train.csv\n",
      "/kaggle/input/title-generation/vocs.pkl\n",
      "/kaggle/input/title-generation/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: allennlp in /opt/conda/lib/python3.6/site-packages (0.9.0)\r\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (from allennlp) (3.2.4)\r\n",
      "Requirement already satisfied: jsonpickle in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.17.4)\r\n",
      "Requirement already satisfied: overrides in /opt/conda/lib/python3.6/site-packages (from allennlp) (2.5)\r\n",
      "Requirement already satisfied: conllu==1.3.1 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.3.1)\r\n",
      "Requirement already satisfied: tensorboardX>=1.2 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.9)\r\n",
      "Requirement already satisfied: numpydoc>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from allennlp) (0.9.1)\r\n",
      "Requirement already satisfied: flask>=1.0.2 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.1.1)\r\n",
      "Requirement already satisfied: torch>=1.2.0 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.3.0)\r\n",
      "Collecting spacy<2.2,>=2.1.0\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/5b/e07dd3bf104237bce4b398558b104c8e500333d6f30eabe3fa9685356b7d/spacy-2.1.9-cp36-cp36m-manylinux1_x86_64.whl (30.8MB)\r\n",
      "\u001b[K     |████████████████████████████████| 30.9MB 36.2MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: word2number>=1.1 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.1)\r\n",
      "Requirement already satisfied: tqdm>=4.19 in /opt/conda/lib/python3.6/site-packages (from allennlp) (4.39.0)\r\n",
      "Requirement already satisfied: flask-cors>=3.0.7 in /opt/conda/lib/python3.6/site-packages (from allennlp) (3.0.8)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from allennlp) (2.9.0)\r\n",
      "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from allennlp) (0.6.2)\r\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.10.29)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from allennlp) (0.21.3)\r\n",
      "Requirement already satisfied: ftfy in /opt/conda/lib/python3.6/site-packages (from allennlp) (5.6)\r\n",
      "Requirement already satisfied: requests>=2.18 in /opt/conda/lib/python3.6/site-packages (from allennlp) (2.22.0)\r\n",
      "Requirement already satisfied: parsimonious>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from allennlp) (0.8.1)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.3.3)\r\n",
      "Requirement already satisfied: pytorch-transformers==1.1.0 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.1.0)\r\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /opt/conda/lib/python3.6/site-packages (from allennlp) (3.0.3)\r\n",
      "Requirement already satisfied: sqlparse>=0.2.4 in /opt/conda/lib/python3.6/site-packages (from allennlp) (0.3.0)\r\n",
      "Requirement already satisfied: unidecode in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.1.1)\r\n",
      "Requirement already satisfied: editdistance in /opt/conda/lib/python3.6/site-packages (from allennlp) (0.5.3)\r\n",
      "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /opt/conda/lib/python3.6/site-packages (from allennlp) (0.14.0)\r\n",
      "Collecting responses>=0.7\r\n",
      "  Downloading https://files.pythonhosted.org/packages/3e/0c/940781dd49710f4b1f0650c450c9fd8491db0e1bffd99ebc36355607f96d/responses-0.10.9-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.6/site-packages (from allennlp) (2019.3)\r\n",
      "Collecting flaky\r\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: pytest in /opt/conda/lib/python3.6/site-packages (from allennlp) (5.0.1)\r\n",
      "Requirement already satisfied: gevent>=1.3.6 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.4.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk->allennlp) (1.13.0)\r\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.6/site-packages (from tensorboardX>=1.2->allennlp) (3.11.0)\r\n",
      "Requirement already satisfied: Jinja2>=2.3 in /opt/conda/lib/python3.6/site-packages (from numpydoc>=0.8.0->allennlp) (2.10.3)\r\n",
      "Requirement already satisfied: sphinx>=1.6.5 in /opt/conda/lib/python3.6/site-packages (from numpydoc>=0.8.0->allennlp) (2.2.1)\r\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/conda/lib/python3.6/site-packages (from flask>=1.0.2->allennlp) (0.16.0)\r\n",
      "Requirement already satisfied: click>=5.1 in /opt/conda/lib/python3.6/site-packages (from flask>=1.0.2->allennlp) (7.0)\r\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/lib/python3.6/site-packages (from flask>=1.0.2->allennlp) (1.1.0)\r\n",
      "Collecting blis<0.3.0,>=0.2.2\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\r\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 24.0MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.3)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.1.0->allennlp) (0.4.2)\r\n",
      "Collecting preshed<2.1.0,>=2.0.1\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\r\n",
      "\u001b[K     |████████████████████████████████| 92kB 7.9MB/s \r\n",
      "\u001b[?25hCollecting plac<1.0.0,>=0.9.6\r\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\r\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.0)\r\n",
      "Collecting thinc<7.1.0,>=7.0.8\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/a5/9ace20422e7bb1bdcad31832ea85c52a09900cd4a7ce711246bfb92206ba/thinc-7.0.8-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\r\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 23.5MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert>=0.6.0->allennlp) (2019.11.1)\r\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.29 in /opt/conda/lib/python3.6/site-packages (from boto3->allennlp) (1.13.29)\r\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->allennlp) (0.2.1)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->allennlp) (0.9.4)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->allennlp) (0.14.0)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from ftfy->allennlp) (0.1.7)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp) (1.24.2)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp) (2.8)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp) (2019.9.11)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp) (3.0.4)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (from pytorch-transformers==1.1.0->allennlp) (0.1.83)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.2.3->allennlp) (2.4.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.2.3->allennlp) (2.8.0)\r\n",
      "Requirement already satisfied: py>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (1.8.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (19.2)\r\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (19.3.0)\r\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (7.2.0)\r\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (1.3.0)\r\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (0.13.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (0.23)\r\n",
      "Requirement already satisfied: greenlet>=0.4.14 in /opt/conda/lib/python3.6/site-packages (from gevent>=1.3.6->allennlp) (0.4.15)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorboardX>=1.2->allennlp) (42.0.1.post20191125)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.6/site-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp) (1.1.1)\r\n",
      "Requirement already satisfied: sphinxcontrib-devhelp in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.0.1)\r\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\r\n",
      "Requirement already satisfied: babel!=2.0,>=1.3 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.7.0)\r\n",
      "Requirement already satisfied: sphinxcontrib-jsmath in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.0.1)\r\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.0.2)\r\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\r\n",
      "Requirement already satisfied: docutils>=0.12 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.15.2)\r\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.3)\r\n",
      "Requirement already satisfied: sphinxcontrib-qthelp in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.0.2)\r\n",
      "Requirement already satisfied: sphinxcontrib-applehelp in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.0.1)\r\n",
      "Requirement already satisfied: Pygments>=2.0 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.4.2)\r\n",
      "Requirement already satisfied: imagesize in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=0.12->pytest->allennlp) (0.6.0)\r\n",
      "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.9 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: en-core-web-lg 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.9 which is incompatible.\u001b[0m\r\n",
      "Installing collected packages: blis, preshed, plac, thinc, spacy, responses, flaky\r\n",
      "  Found existing installation: blis 0.4.1\r\n",
      "    Uninstalling blis-0.4.1:\r\n",
      "      Successfully uninstalled blis-0.4.1\r\n",
      "  Found existing installation: preshed 3.0.2\r\n",
      "    Uninstalling preshed-3.0.2:\r\n",
      "      Successfully uninstalled preshed-3.0.2\r\n",
      "  Found existing installation: plac 1.1.3\r\n",
      "    Uninstalling plac-1.1.3:\r\n",
      "      Successfully uninstalled plac-1.1.3\r\n",
      "  Found existing installation: thinc 7.3.1\r\n",
      "    Uninstalling thinc-7.3.1:\r\n",
      "      Successfully uninstalled thinc-7.3.1\r\n",
      "  Found existing installation: spacy 2.2.3\r\n",
      "    Uninstalling spacy-2.2.3:\r\n",
      "      Successfully uninstalled spacy-2.2.3\r\n",
      "Successfully installed blis-0.2.4 flaky-3.6.1 plac-0.9.6 preshed-2.0.1 responses-0.10.9 spacy-2.1.9 thinc-7.0.8\r\n"
     ]
    }
   ],
   "source": [
    "!pip install allennlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.6/site-packages (2.1.9)\r\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /opt/conda/lib/python3.6/site-packages (from spacy) (0.9.6)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.6/site-packages (from spacy) (1.17.4)\r\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from spacy) (0.2.4)\r\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /opt/conda/lib/python3.6/site-packages (from spacy) (7.0.8)\r\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /opt/conda/lib/python3.6/site-packages (from spacy) (0.2.0)\r\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from spacy) (2.0.1)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy) (2.0.3)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from spacy) (0.4.2)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.6/site-packages (from spacy) (1.0.2)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.6/site-packages (from spacy) (2.22.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.6/site-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.39.0)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Dict, List\n",
    "import itertools\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from allennlp.data import Tokenizer, Token\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import torch.nn as nn\n",
    "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper\n",
    "import spacy\n",
    "import pickle\n",
    "from allennlp.data.token_indexers.token_characters_indexer import TokenCharactersIndexer\n",
    "from allennlp.data import DatasetReader, Instance, Tokenizer, TokenIndexer, Token\n",
    "from allennlp.data.fields import TextField, ArrayField, MetadataField, SequenceLabelField\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.models.encoder_decoders.simple_seq2seq import SimpleSeq2Seq\n",
    "from allennlp.data.dataset_readers.seq2seq import Seq2SeqDatasetReader\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.attention import LinearAttention, BilinearAttention, DotProductAttention\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.common import Params\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "from allennlp.predictors import SimpleSeq2SeqPredictor\n",
    "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper, StackedSelfAttentionEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we consider the problem of utility maximizatio...</td>\n",
       "      <td>on optimal investment with processes of long o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in this paper we provide an explicit formula f...</td>\n",
       "      <td>boolean complexes for ferrers graphs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kinesin-5, also known as eg5 in vertebrates is...</td>\n",
       "      <td>relative velocity of sliding of microtubules b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we discuss the transition paths in a coupled b...</td>\n",
       "      <td>bifurcation of transition paths induced by cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>two types of room temperature detectors of ter...</td>\n",
       "      <td>all-electric detectors of the polarization sta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  we consider the problem of utility maximizatio...   \n",
       "1  in this paper we provide an explicit formula f...   \n",
       "2  kinesin-5, also known as eg5 in vertebrates is...   \n",
       "3  we discuss the transition paths in a coupled b...   \n",
       "4  two types of room temperature detectors of ter...   \n",
       "\n",
       "                                               title  \n",
       "0  on optimal investment with processes of long o...  \n",
       "1               boolean complexes for ferrers graphs  \n",
       "2  relative velocity of sliding of microtubules b...  \n",
       "3  bifurcation of transition paths induced by cou...  \n",
       "4  all-electric detectors of the polarization sta...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('/kaggle/input/title-generation/train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Most sequence transformation models use recurr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The doc2vec approach was introduced as an exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM models can vary greatly depending on sequ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A joint learning process of alignment and tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Current unsupervised image-to-image translatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract\n",
       "0  Most sequence transformation models use recurr...\n",
       "1  The doc2vec approach was introduced as an exte...\n",
       "2  LSTM models can vary greatly depending on sequ...\n",
       "3  A joint learning process of alignment and tran...\n",
       "4  Current unsupervised image-to-image translatio..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('/kaggle/input/title-generation/test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivTitleDatasetReader(Seq2SeqDatasetReader):\n",
    "    \"\"\"\n",
    "    DatasetReader for Arxiv Papers title generation\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 tokenizer: Tokenizer,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 lazy: bool = False):\n",
    "        super().__init__(lazy=lazy)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer(\"words\", lowercase_tokens=True)}\n",
    "\n",
    "    def text_to_instance(self,\n",
    "                         abstract_tokens: List[Token],\n",
    "                         title_tokens: List[Token] = None) -> Instance:\n",
    "        fields = dict()\n",
    "        if title_tokens is not None:\n",
    "            fields[\"target_tokens\"] = TextField(title_tokens, token_indexers=self.token_indexers)\n",
    "        fields[\"source_tokens\"] = TextField(abstract_tokens, token_indexers=self.token_indexers)\n",
    "        return Instance(fields)\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterable[Instance]:\n",
    "        df = pd.read_csv(file_path)\n",
    "        for i, (row) in df.iterrows():\n",
    "            abstract_raw = row[\"abstract\"]\n",
    "            abstract_tokens = self.tokenizer.tokenize(abstract_raw)\n",
    "            title = row.get(\"title\")\n",
    "            title_tokens = None\n",
    "            if title is not None:\n",
    "                title_tokens = self.tokenizer.tokenize(title)\n",
    "            yield self.text_to_instance(abstract_tokens, title_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/kaggle/input/title-generation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLTKTokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokenizer = ToktokTokenizer()\n",
    "\n",
    "    def tokenize(self, text: str) -> List[Token]:\n",
    "        return [Token(token) for token in self.tokenizer.tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.1.0\r\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\r\n",
      "\u001b[K     |████████████████████████████████| 11.1MB 1.8MB/s \r\n",
      "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\r\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.1.0-cp36-none-any.whl size=11074435 sha256=9e7d7ad81a290e54a1ddb9c849df55c8ea7b7d80310f97a42894144876303524\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hzaqobyh/wheels/39/ea/3b/507f7df78be8631a7a3d7090962194cf55bc1158572c0be77f\r\n",
      "Successfully built en-core-web-sm\r\n",
      "Installing collected packages: en-core-web-sm\r\n",
      "  Found existing installation: en-core-web-sm 2.2.5\r\n",
      "    Uninstalling en-core-web-sm-2.2.5:\r\n",
      "      Successfully uninstalled en-core-web-sm-2.2.5\r\n",
      "Successfully installed en-core-web-sm-2.1.0\r\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\r\n",
      "You can now load the model via spacy.load('en_core_web_sm')\r\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\r\n",
      "/opt/conda/lib/python3.6/site-packages/en_core_web_sm -->\r\n",
      "/opt/conda/lib/python3.6/site-packages/spacy/data/en\r\n",
      "You can now load the model via spacy.load('en')\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spacyTokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokenizer = spacy.load('en')\n",
    "\n",
    "    def tokenize(self, text: str) -> List[Token]:\n",
    "        return [Token(tok.text) for tok in self.tokenizer(text) if not tok.text.isspace()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token_indexer = {\"tokens\": SingleIdTokenIndexer(lowercase_tokens=True), \"chars\": TokenCharactersIndexer(\"chars\", min_padding_length=3)}\n",
    "tokenizer = spacyTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the first time execution if we didnt got\n",
    "# reader = ArxivTitleDatasetReader(tokenizer = tokenizer)\n",
    "\n",
    "# full_train_dataset = reader.read(path + 'train.csv')\n",
    "# pickle.dump(full_train_dataset, open('train_reader.p', 'wb'))\n",
    "\n",
    "\n",
    "# train_dataset, validation_dataset = train_test_split(full_train_dataset, test_size=0.10, random_state=SEED)\n",
    "\n",
    "# test_dataset = reader.read(path + 'test.csv')\n",
    "# pickle.dump(test_dataset, open('test_reader.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the next times execution if we create readers files\n",
    "reader = ArxivTitleDatasetReader(tokenizer = tokenizer)\n",
    "\n",
    "full_train_dataset = pickle.load(open('../input/title-generation-readers/train_reader.p', 'rb'))\n",
    "\n",
    "\n",
    "train_dataset, validation_dataset = train_test_split(full_train_dataset, test_size=0.10, random_state=SEED)\n",
    "\n",
    "test_dataset = pickle.load(open('../input/title-generation-readers/test_reader.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 136000/136000 [00:35<00:00, 3782.74it/s]\n",
      "400000it [00:02, 168125.41it/s]\n"
     ]
    }
   ],
   "source": [
    "emb_filename = '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'\n",
    "\n",
    "vocab = Vocabulary.from_instances(train_dataset + validation_dataset + test_dataset, \n",
    "                                  pretrained_files = {'words': emb_filename},\n",
    "                                  only_include_pretrained_words=False, min_count={\"words\": 7})\n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 256\n",
    "CUDA_DEVICE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target_tokens': {'tokens': tensor([  134,  5867,     9,  3098, 11149,    32,    17,  4101,   950,    45,\n",
      "            7,    97,    58])}, 'source_tokens': {'tokens': tensor([   19,    44,   974,     7,    97,    58,     3,     7,  5559,    83,\n",
      "           14,    30,    26,    67,    10,  7023,   134,     9,  3098, 11149,\n",
      "          314,  8410,     2, 13329,     3,     2, 13462,     4,    11,    93,\n",
      "           24,    61,    14,   757,  4101,   262,    28,   113,     6,   757,\n",
      "          445,    71,    10,   658,  1806,    13, 13462,    10,   927,     2,\n",
      "          134,  1606,     3,    73, 11149,     4,     2,    83, 23299,    13,\n",
      "         4016,     6,  1046,   442,     9,     2,   992,    28,   542,    22,\n",
      "            2,     1,  5582,  6989,    83,     4,    39,   442,    20,  5286,\n",
      "          115,  1039,  1042,     5, 17298,     6,   263,    16,     2,   393,\n",
      "          262,     3,     2, 13462,     4,   140,     2,    83,  5735,    24,\n",
      "         3315,    10,  7023,   134,   314,  2329,     2, 13329,   196,    34,\n",
      "         9742,     7,  3928,    10,     2, 11386,     4])}}\n",
      "Vocabulary with namespaces:\n",
      " \tNon Padded Namespaces: {'*labels', '*tags'}\n",
      " \tNamespace: words, Size: 74590 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "instance = train_dataset[0]\n",
    "instance.index_fields(vocab)\n",
    "print(instance.as_tensor_dict())\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:08, 46288.71it/s] \n"
     ]
    }
   ],
   "source": [
    "emb_params = Params({\"pretrained_file\": emb_filename, \n",
    "                     \"embedding_dim\": 200,\n",
    "                     \"trainable\": True,\n",
    "                     \"vocab_namespace\": \"words\"})\n",
    "token_embedding = Embedding.from_params(vocab, emb_params)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = PytorchSeq2SeqWrapper(nn.LSTM(token_embedding.get_output_dim(), HIDDEN_DIM, batch_first=True, bidirectional=True))\n",
    "\n",
    "\n",
    "attention = DotProductAttention()\n",
    "\n",
    "max_decoding_steps = 20   # TODO: make this variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SimpleSeq2Seq(vocab, word_embeddings,\n",
    "#                       encoder, max_decoding_steps,\n",
    "#                       target_embedding_dim=EMBEDDING_DIM,\n",
    "#                       target_namespace='words',\n",
    "#                       attention=attention,\n",
    "#                       beam_size=8,\n",
    "#                       use_bleu=True,\n",
    "#                       scheduled_sampling_ratio = 0.5\n",
    "#                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../input/allen-nlp-baseline/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cuda_device = 0\n",
    "    model = model.cuda(cuda_device)\n",
    "else:\n",
    "    cuda_device = -1\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "iterator = BucketIterator(batch_size=32, sorting_keys=[(\"source_tokens\", \"num_tokens\")])\n",
    "iterator.index_with(vocab)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=validation_dataset,\n",
    "                  num_epochs=30,\n",
    "                  cuda_device=CUDA_DEVICE,\n",
    "                  patience = 3,\n",
    "                  validation_metric=\"+BLEU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': array([[ 5867,     9,  3098, 11149,    27,  4101,   950,    45,     7,\n",
       "            97,    58,    17,    97,    58,    17,    97,    58,     9,\n",
       "            97,    58],\n",
       "        [ 5867,     9,  3098, 11149,    32,    17,  4101,   950,    45,\n",
       "             7,    97,    58,     6,    97,    58,     9,    97,    58,\n",
       "             9,    97],\n",
       "        [ 5867,     9,  3098, 11149,    32,    17,  4101,   950,    45,\n",
       "             7,    97,    58,     6,    97,    58,     9,    97,    58,\n",
       "             9,  3098],\n",
       "        [ 5867,     9,  3098, 11149,    32,    17,  4101,   950,    45,\n",
       "             7,    97,    58,     6,    97,    58,    17,    97,    58,\n",
       "             9,    97],\n",
       "        [ 5867,     9,  3098, 11149,    32,    17,  4101,   950,    45,\n",
       "             7,    97,    58,    17,    97,    58,     9,    97,    58,\n",
       "             9,    97],\n",
       "        [ 5867,     9,  3098, 11149,    32,    17,  4101,   950,    45,\n",
       "             7,    97,    58,    17,    97,    58,     9,    97,    58,\n",
       "             9,  3098],\n",
       "        [ 5867,     9,  3098, 11149,    27,  4101,   950,    45,     7,\n",
       "            97,    58,    17,     2,    97,    58,    17,    97,    58,\n",
       "             9,    97],\n",
       "        [ 5867,     9,  3098, 11149,    27,  4101,   950,    45,     7,\n",
       "            97,    58,    17,    97,    58,    17,    97,    58,     9,\n",
       "            97,    97]]),\n",
       " 'loss': 0.19005793,\n",
       " 'class_log_probabilities': array([ -9.87354 , -10.866399, -10.917344, -11.045679, -11.056874,\n",
       "        -11.113083, -11.357853, -11.419227], dtype=float32),\n",
       " 'predicted_tokens': ['saving',\n",
       "  'in',\n",
       "  'smart',\n",
       "  'homes',\n",
       "  'from',\n",
       "  'consumer',\n",
       "  'behaviour',\n",
       "  ':',\n",
       "  'a',\n",
       "  'case',\n",
       "  'study',\n",
       "  'on',\n",
       "  'case',\n",
       "  'study',\n",
       "  'on',\n",
       "  'case',\n",
       "  'study',\n",
       "  'in',\n",
       "  'case',\n",
       "  'study']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward_on_instance(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.7244 ||: 100%|██████████| 3797/3797 [11:44<00:00,  5.39it/s]\n",
      "BLEU: 0.1495, loss: 4.8469 ||: 100%|██████████| 422/422 [02:43<00:00,  2.58it/s]\n",
      "loss: 0.6752 ||: 100%|██████████| 3797/3797 [10:58<00:00,  5.76it/s]\n",
      "BLEU: 0.1518, loss: 4.8926 ||: 100%|██████████| 422/422 [02:39<00:00,  2.64it/s]\n",
      "loss: 0.6605 ||: 100%|██████████| 3797/3797 [10:58<00:00,  5.77it/s]\n",
      "BLEU: 0.1530, loss: 4.8934 ||: 100%|██████████| 422/422 [02:40<00:00,  2.63it/s]\n",
      "loss: 0.6526 ||: 100%|██████████| 3797/3797 [10:56<00:00,  5.78it/s]\n",
      "BLEU: 0.1518, loss: 4.9708 ||: 100%|██████████| 422/422 [02:39<00:00,  2.64it/s]\n",
      "BLEU: 0.1598, loss: 5.0877 ||: 100%|██████████| 422/422 [02:37<00:00,  2.68it/s]\n",
      "loss: 0.5497 ||:  76%|███████▋  | 2902/3797 [08:25<03:07,  4.77it/s]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type SimpleSeq2Seq. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BasicTextFieldEmbedder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type PytorchSeq2SeqWrapper. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DotProductAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTMCell. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load('../input/kernel5317bc3674/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': array([[   11,   187,    10,   568,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1],\n",
       "        [   54,    10,     2,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1],\n",
       "        [   11,   187,    10,     2,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1],\n",
       "        [   11,   187,    10,   568,     2,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1],\n",
       "        [   11,   187,    10,   275,     3,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1],\n",
       "        [   11,   187,    10,   568,   347,    10, 18257,  1774,  2019,\n",
       "             9,   780,  3481,    45,     7,   226,     5,     6,    24,\n",
       "           279,    10],\n",
       "        [   11,   187,    10,   568,   347,    10, 18257,  1774,  2019,\n",
       "             9,   780,  3481,    45,     7,   226,     5,     6,     5,\n",
       "          1774,  2019],\n",
       "        [   11,   187,    10,   568,   347,    10, 18257,  1774,  2019,\n",
       "             9,   780,  3481,    45,     7,   226,     5,     6,     5,\n",
       "             6,  1774]]),\n",
       " 'loss': 1.3307718,\n",
       " 'class_log_probabilities': array([ -4.0143185,  -4.070009 ,  -4.6786327,  -4.8583984,  -6.1900434,\n",
       "        -17.161892 , -17.33916  , -18.127533 ], dtype=float32),\n",
       " 'predicted_tokens': ['we', 'test', 'to', 'identify']}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward_on_instance(train_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "\n",
    "#submission_data = pd.read_csv('/kaggle/input/title-generation/test.csv')\n",
    "#abstracts = submission_data['abstract'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv(model, dataset, df):\n",
    "    model.eval()\n",
    "    batch_size = 32\n",
    "    column = []\n",
    "    for i in range(len(dataset) // batch_size + 1):\n",
    "        batch = dataset[i * batch_size: (i + 1) * batch_size]\n",
    "        predictions = model.forward_on_instances(batch)\n",
    "        for pred in predictions:\n",
    "            column.append(' '.join(pred[\"predicted_tokens\"]))\n",
    "    df[\"title\"] = column\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../input/title-generation/test.csv\")\n",
    "df = get_csv(model, test_dataset, test_df)\n",
    "df.fillna(' ', inplace = True)\n",
    "df.to_csv(\"res.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Most sequence transformation models use recurr...</td>\n",
       "      <td>: deep neural networks : encoder - decoder - f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The doc2vec approach was introduced as an exte...</td>\n",
       "      <td>program with a data - simple and simple contex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM models can vary greatly depending on sequ...</td>\n",
       "      <td>for vertex using symbolic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A joint learning process of alignment and tran...</td>\n",
       "      <td>background - based description of alignment an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Current unsupervised image-to-image translatio...</td>\n",
       "      <td>image synthesis with image model image transla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  Most sequence transformation models use recurr...   \n",
       "1  The doc2vec approach was introduced as an exte...   \n",
       "2  LSTM models can vary greatly depending on sequ...   \n",
       "3  A joint learning process of alignment and tran...   \n",
       "4  Current unsupervised image-to-image translatio...   \n",
       "\n",
       "                                               title  \n",
       "0  : deep neural networks : encoder - decoder - f...  \n",
       "1  program with a data - simple and simple contex...  \n",
       "2                          for vertex using symbolic  \n",
       "3  background - based description of alignment an...  \n",
       "4  image synthesis with image model image transla...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "def generate_csv(input_file='res.csv',\n",
    "                 output_file='submission.csv',\n",
    "                 voc_file='/kaggle/input/title-generation/vocs.pkl'):\n",
    "    '''\n",
    "    Generates file in format required for submitting result to Kaggle\n",
    "    \n",
    "    Parameters:\n",
    "        input_file (str) : path to csv file with your predicted titles.\n",
    "                           Should have two fields: abstract and title\n",
    "        output_file (str) : path to output submission file\n",
    "        voc_file (str) : path to voc.pkl file\n",
    "    '''\n",
    "    data = pd.read_csv(input_file)\n",
    "    with open(voc_file, 'rb') as voc_file:\n",
    "        vocs = pickle.load(voc_file)\n",
    "\n",
    "    with open(output_file, 'w') as res_file:\n",
    "        res_file.write('Id,Predict\\n')\n",
    "        \n",
    "    output_idx = 0\n",
    "    for row_idx, row in data.iterrows():\n",
    "        #print(row)\n",
    "        trg = row[0]\n",
    "        trg = trg.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
    "        trg.extend(['_'.join(ngram) for ngram in list(ngrams(trg, 2)) + list(ngrams(trg, 3))])\n",
    "        \n",
    "        VOCAB_stoi = vocs[row_idx]\n",
    "        trg_intersection = set(VOCAB_stoi.keys()).intersection(set(trg))\n",
    "        trg_vec = np.zeros(len(VOCAB_stoi))    \n",
    "\n",
    "        for word in trg_intersection:\n",
    "            trg_vec[VOCAB_stoi[word]] = 1\n",
    "\n",
    "        with open(output_file, 'a') as res_file:\n",
    "            for is_word in trg_vec:\n",
    "                res_file.write('{0},{1}\\n'.format(output_idx, int(is_word)))\n",
    "                output_idx += 1\n",
    "\n",
    "\n",
    "generate_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='submission.csv' target='_blank'>submission.csv</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/submission.csv"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# res = pd.read_csv(\"../input/res.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
